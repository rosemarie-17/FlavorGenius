{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Grocery_and_Gourmet_Food\", trust_remote_code=True)\n",
    "print(dataset['full'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = dataset['full'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item Metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_Grocery_and_Gourmet_Food\", trust_remote_code=True)\n",
    "print(dataset['full'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = dataset['full'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df_reviews.drop(columns=['images'], axis=1)\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.drop(columns=['images'], axis=1)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.drop(columns=['videos'], axis=1)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find items using categories & parent_asin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta['categories'] = df_meta['categories'].apply(lambda x: tuple(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "df_meta[['categories', 'parent_asin']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carb = df_meta[df_meta['title'].str.contains('low carb', case=False, na=False)]\n",
    "parent_asin_list_carb = df_carb['parent_asin'].drop_duplicates().to_list()\n",
    "len(parent_asin_list_carb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carb.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snacks = df_meta[df_meta['title'].str.contains('snack', case=False, na=False)]\n",
    "parent_asin_list_snacks = df_snacks['parent_asin'].drop_duplicates().to_list()\n",
    "len(parent_asin_list_snacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snacks.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_asin_list = parent_asin_list_carb + parent_asin_list_snacks\n",
    "df_healthy = df_reviews[df_reviews['parent_asin'].isin(parent_asin_list)]\n",
    "df_healthy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_meta.rename(columns={'parent_asin': 'parent_asin_meta'})\n",
    "joined_df = pd.merge(df_healthy, df_meta, left_on=\"parent_asin\", right_on=\"parent_asin_meta\", how=\"left\")\n",
    "joined_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stop words and tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize and remove punctuation\n",
    "    tokens = simple_preprocess(text, deacc=True)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df['cleaned_text'] = joined_df['text'].apply(preprocess_text)\n",
    "joined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_reviews = joined_df['cleaned_text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(joined_df['cleaned_text'].iloc[0], list):\n",
    "    processed_reviews = joined_df['cleaned_text'].tolist()\n",
    "else:\n",
    "    raise ValueError(\"Ensure 'processed_reviews' contains lists of tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(processed_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(review) for review in processed_reviews]\n",
    "total_docs = len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lda_training(corpus, dictionary, num_topics, passes, chunksize):\n",
    "    num_iterations = passes\n",
    "\n",
    "    with tqdm(total=total_docs * num_iterations, desc=\"LDA Training Progress\") as pbar:\n",
    "        # Initialize the LDA Multicore model\n",
    "        lda_model = LdaMulticore(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            random_state=100,\n",
    "            chunksize=chunksize,\n",
    "            passes=1,\n",
    "            workers=1,\n",
    "            alpha='symmetric',\n",
    "            per_word_topics=True,\n",
    "            batch=True\n",
    "        )\n",
    "\n",
    "        # Manually iterate over the passes and update the progress bar\n",
    "        for pass_ in range(num_iterations):\n",
    "            for i in range(0, total_docs, chunksize):\n",
    "                chunk = corpus[i:i + chunksize]\n",
    "                lda_model.update(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = custom_lda_training(corpus, dictionary, num_topics=20, passes=5, chunksize=3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
